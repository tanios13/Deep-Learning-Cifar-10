{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe49c202",
   "metadata": {},
   "source": [
    "# K-layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3085c3f3",
   "metadata": {},
   "source": [
    "We will train and test a k layer network with multiple outputs to classify images from the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad740786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions to load batch, softmax function, compute gradient, display image for each label\n",
    "# and transfer model to matlab\n",
    "import functions as functions\n",
    "\n",
    "import tensorflow.keras.utils as np_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc9dcebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter of the network\n",
    "k = 3\n",
    "m = [50, 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e68052",
   "metadata": {},
   "source": [
    "# Training the multi-linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67932793",
   "metadata": {},
   "source": [
    "## Read in the data & initialize the parameters of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73119a",
   "metadata": {},
   "source": [
    "We start by extracting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1691a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads in the data from a CIFAR-10 batch file \n",
    "# Returns the image and label data in separate files.\n",
    "def LoadBatch(filename):\n",
    "    data_dict = functions.load_batch(filename)\n",
    "    data = data_dict[b'data']\n",
    "    \n",
    "    # extract the labels\n",
    "    labels = data_dict[b'labels']\n",
    "    \n",
    "    # convert to one-hot representation\n",
    "    onehot_labels = np_utils.to_categorical(labels)\n",
    "    \n",
    "    return data.T, onehot_labels.T, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c31ae",
   "metadata": {},
   "source": [
    "Now we preprocess the raw data by normalizing it (we assume the noise is Gaussian and that the data is normally distributed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "375919e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X, eps=1e-14):    \n",
    "    return (X -  np.mean(X,axis=1)[:, np.newaxis]) / (np.std(X, axis=1)[:, np.newaxis] + eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d69e94",
   "metadata": {},
   "source": [
    "We know test the code until now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37cb5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data mean: -5.164387436214686e-18\n",
      "data std: 0.9999999999999997\n",
      "validation mean: -5.164387436214686e-18\n",
      "validation std: 0.9999999999999997\n"
     ]
    }
   ],
   "source": [
    "# dir of data\n",
    "data_batch_1 = \"cifar-10-batches-py/data_batch_1\"\n",
    "# validation data\n",
    "data_batch_2 = \"cifar-10-batches-py/data_batch_2\"\n",
    "\n",
    "# data is (data_dimension, data_count)\n",
    "# onehot_labels is (labels_count, data_count)\n",
    "data, onehot_labels, labels = LoadBatch(data_batch_1)\n",
    "\n",
    "# we do the same for the validation set\n",
    "X_validation, Y_validation, labels_validation = LoadBatch(data_batch_2)\n",
    "\n",
    "data_dimension = len(data[:,0])\n",
    "labels_count = len(onehot_labels[:,0])\n",
    "\n",
    "# Than normalize the data according to the normal distribution\n",
    "data = normalize_data(data)\n",
    "X_validation = normalize_data(X_validation)\n",
    "\n",
    "print(\"data mean:\", np.mean(data))\n",
    "print(\"data std:\", np.std(data))\n",
    "\n",
    "print(\"validation mean:\", np.mean(data))\n",
    "print(\"validation std:\", np.std(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c219fe3",
   "metadata": {},
   "source": [
    "For $l = 1,2...k-1$:\n",
    "$$ s^l = W_lx^{l-1} + b_l$$\n",
    "$$x^l = max(0,s^l)$$\n",
    "and then finally we have:\n",
    "$$s = W_kx^{k-1} + b_k$$\n",
    "$$p = softmax(s)$$\n",
    "We know initialize the W's and b's of the model with each entry have Gaussian random values with zero mean and standard\n",
    "deviation according to the Xavier initialization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d30874bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the W's\n",
    "def initialize_Ws(k, m, data_dimension, labels_count):\n",
    "    m = m.copy()\n",
    "    \n",
    "    m.insert(0, data_dimension)\n",
    "    m.append(labels_count)\n",
    "    Ws = []\n",
    "    bs = []\n",
    "    \n",
    "    for i in range(0, k):\n",
    "        Wi_shape = (m[i+1], m[i])\n",
    "        Wi = np.random.normal(0, np.sqrt(1/m[i]), Wi_shape)\n",
    "        Ws.append(Wi)\n",
    "        bs.append(np.zeros(m[i+1])[:,np.newaxis])\n",
    "        \n",
    "    return Ws, bs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0d6b219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ws, bs = initialize_Ws(k, m, data_dimension, labels_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b78cd1",
   "metadata": {},
   "source": [
    "# Forward pass and the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e7e10",
   "metadata": {},
   "source": [
    "We write the evaluation and cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d11d15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier_BN(data, Ws, bs, mean_test, var_test, gamma, beta, test):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \n",
    "    data: (N, data_size) if training else (N, 1) during test\n",
    "    \n",
    "    test: 0 if we evaluate during training time and 1 during test time\n",
    "    \n",
    "    \"\"\"\n",
    "    xs = [data]\n",
    "    s = []\n",
    "    s_hat = []\n",
    "    k = len(Ws)\n",
    "    means = []\n",
    "    var = []\n",
    "    \n",
    "    # forward through the k layers\n",
    "    for i in range(0,k-1):\n",
    "        s_i = Ws[i]@xs[i] + bs[i]\n",
    "        s.append(s_i)\n",
    "        \n",
    "        # normalize, scale and shift\n",
    "        s_hat_i, mean_i, var_i = batch_normalize_scale_shift(s_i, mean_test[i], var_test[i], gamma[i], beta[i], test)\n",
    "        s_hat.append(s_hat_i)\n",
    "            \n",
    "        xs.append(np.maximum(0, s_hat_i))\n",
    "        means.append(mean_i)\n",
    "        var.append(var_i)\n",
    "        \n",
    "    # forward through the last layer            \n",
    "    p = functions.softmax(Ws[k-1]@xs[k-1] + bs[k-1])\n",
    "    \n",
    "    return xs, p, s, s_hat, means, var\n",
    "\n",
    "def batch_normalize_scale_shift(x, mean_test, var_test, gamma, beta, test):\n",
    "    \"\"\"\n",
    "    Normalize, scale and shift the data\n",
    "    _________\n",
    "    Arguments:\n",
    "    \n",
    "    data: (N, data_size) if training else (N, 1) during test\n",
    "    \n",
    "    \"\"\"\n",
    "    # normalize\n",
    "    x, mean, var = batch_normalize(x, mean_test, var_test, test)\n",
    "                \n",
    "    # scale and shift\n",
    "    x = x * gamma[:, np.newaxis] + beta[:, np.newaxis]\n",
    "    \n",
    "    return x, mean, var\n",
    "\n",
    "# normalize the data with mean and var\n",
    "def batch_normalize(x, mean_test, var_test, test):\n",
    "    epsilon = 1e-14\n",
    "\n",
    "    # compute var and mean\n",
    "    mean = np.mean(x, axis=1) if test == 0 else mean_test\n",
    "    var = np.var(x, axis=1)  if test == 0 else var_test\n",
    "    x = (x -  mean[:, np.newaxis]) / np.sqrt((var[:, np.newaxis] + epsilon))\n",
    "         \n",
    "    return x, mean, var\n",
    "\n",
    "# returns the cost functions that we need to minimize\n",
    "def compute_cost(data, onehot_labels, Ws, bs, lbd, gamma, beta, mean_test, var_test, test):\n",
    "    cost = 0\n",
    "\n",
    "    # p is (label_count, data_count)\n",
    "    xs, p, s, s_hat, means, var = evaluate_classifier_BN(data, Ws, bs, mean_test, var_test, gamma, beta, test)\n",
    "        \n",
    "    # for every data in training data set\n",
    "    for d in range(0,len(data[0])):\n",
    "        cost -= onehot_labels[:,d].T @ np.log(p[:,d])\n",
    "        \n",
    "    # we devide by the data_size    \n",
    "    cost /= len(data[0])\n",
    "    \n",
    "    # we add the regularization term\n",
    "    for i in range(0,k):\n",
    "        cost += lbd * np.sum(Ws[i]**2)\n",
    "         \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24729429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.436485411039018"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = [np.ones(m[i]) for i in range(k-1)]\n",
    "beta = [np.zeros(m[i]) for i in range(k-1)]\n",
    "compute_cost(data, onehot_labels, Ws, bs, 0, gamma, beta, np.zeros(k), np.zeros(k), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1042a916",
   "metadata": {},
   "source": [
    "We write a function that computes the accuracy of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e2c7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(data, labels, Ws, bs, gamma, beta, means, var, test):    \n",
    "    # Get the index of the maximum value which is the label for each row\n",
    "    xs, p, s, s_hat, means, var = evaluate_classifier_BN(data, Ws, bs, means, var, gamma, beta, test)\n",
    "    predicted_labels = np.argmax(p, axis=0)\n",
    "    return np.sum(labels == predicted_labels) / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb449d",
   "metadata": {},
   "source": [
    "We compute the accuracy for the randomly initialized parameters. We should get an accuracy of 10% since it's random and there is 10 labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eec6a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1125"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_accuracy(data, labels, Ws, bs, gamma, beta, np.zeros(k), np.zeros(k), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1972ce48",
   "metadata": {},
   "source": [
    "## Compute the gradients for the network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9cd4cc",
   "metadata": {},
   "source": [
    "We write the function that evaluates for a mini-batch the gradient of the cost function.\n",
    "\n",
    "The mini-batch gradient is defined as follows: \n",
    "\n",
    "$$\\textbf{W}^{t+1} = \\textbf{W}^t - \\eta \\sum_{n \\in B^t} \\nabla l_{cross}(\\textbf{x},\\textbf{y},\\textbf{W},\\textbf{b})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccc694b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(onehot_labels, predicted_labels, Ws, xs, s, s_hat, gamma, means, var, lbd):\n",
    "    data_size = len(xs[0][0])\n",
    "    \n",
    "    grad_Ws = []\n",
    "    grad_bs = []\n",
    "    \n",
    "    # gradient for scale and shift\n",
    "    grad_gamma = []\n",
    "    grad_beta = []\n",
    "            \n",
    "    # g is (label_count, data_size)\n",
    "    # We start by the last layer\n",
    "    g = -(onehot_labels - predicted_labels)\n",
    "    \n",
    "    grad_Wi = (g@xs[k-1].T)/data_size + 2*lbd*Ws[k-1]\n",
    "    grad_bi = np.mean(g, axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    grad_Ws.append(grad_Wi)\n",
    "    grad_bs.append(grad_bi)\n",
    "        \n",
    "    g = Ws[k-1].T@g\n",
    "    g = g * (xs[k-1] > 0).astype(int)\n",
    "    \n",
    "    # now the rest of the layers\n",
    "    for i in range(k-2, -1, -1):\n",
    "        grad_gamma_i = np.mean(g * s_hat[i], axis=1)\n",
    "        grad_beta_i = np.mean(g, axis = 1)  \n",
    "        grad_gamma.append(grad_gamma_i)\n",
    "        grad_beta.append(grad_beta_i)\n",
    "        \n",
    "        # propagate the gradients through scale and shift\n",
    "        g = g*(gamma[i][:,np.newaxis])\n",
    "        g = batch_norm_back_pass(g, s[i], means[i], var[i], data_size)\n",
    "        \n",
    "        grad_bi = np.mean(g, axis = 1)[:, np.newaxis]\n",
    "        grad_Wi = (g@xs[i].T)/data_size + 2*lbd*Ws[i]\n",
    "        grad_Ws.append(grad_Wi)\n",
    "        grad_bs.append(grad_bi)\n",
    "        \n",
    "        g = Ws[i].T@g\n",
    "        g = g * (xs[i] > 0).astype(int)\n",
    "        \n",
    "     \n",
    "    grad_Ws.reverse()\n",
    "    grad_bs.reverse()   \n",
    "    grad_gamma.reverse()\n",
    "    grad_beta.reverse()\n",
    "    \n",
    "    return grad_Ws, grad_bs, grad_gamma, grad_beta\n",
    "\n",
    "def batch_norm_back_pass(g, s, mean, var, data_size):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "    \n",
    "    mean: (N, data_size)\n",
    "    var: (N, data_size)\n",
    "    \n",
    "    \"\"\"\n",
    "    n_1 = np.ones((data_size,1))\n",
    "    \n",
    "    eps = 1e-14\n",
    "    std_1 = np.power(var + eps, -0.5)[:,np.newaxis]\n",
    "    std_2 = np.power(var + eps, -1.5)[:,np.newaxis]\n",
    "\n",
    "    g1 = g * std_1\n",
    "    g2 = g * std_2\n",
    "        \n",
    "    D = s - mean[:,np.newaxis]\n",
    "    c = (g2 * D) @ n_1\n",
    "    \n",
    "    return g1 - ((g1 @ n_1) @ n_1.T)/data_size - (D * (c @ n_1.T))/data_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200487e0",
   "metadata": {},
   "source": [
    "## Train the network with cyclical learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df71d3d",
   "metadata": {},
   "source": [
    "Now that we made sure the gradient descent it correct, we implement the mini batch gardient algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0224f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd_params = (s_batch, eta_min, eta_max, step_size, n_epochs)\n",
    "def mini_batch_gd(data, onehot_labels, gd_params, Ws, bs, gamma, beta, lbd, X_validation, Y_validation, alpha=0.9):\n",
    "    # define the parameters\n",
    "    s_batch = gd_params[0]\n",
    "    eta_min, eta_max, step_size = gd_params[1], gd_params[2], gd_params[3]\n",
    "    n_epochs = gd_params[4]\n",
    "    \n",
    "    # initialize empty lists to store the loss and cost function values\n",
    "    cost = []\n",
    "    cost_vald = []\n",
    "    accuracy = []\n",
    "    accuracy_vald = []\n",
    "    \n",
    "    Ws_star = Ws.copy()\n",
    "    bs_star = bs.copy()\n",
    "    gamma_star = gamma.copy()\n",
    "    beta_star = beta.copy()\n",
    "    \n",
    "    \n",
    "    # cyclical learning rate\n",
    "    step_t = 0\n",
    "    l = 0\n",
    "    \n",
    "    # construct the mini batches\n",
    "    mini_batches = construct_mini_batches(s_batch, data, onehot_labels)     \n",
    "    validation_mini_batches = construct_mini_batches(s_batch, X_validation, Y_validation)\n",
    "     \n",
    "    for iter in range(n_epochs):\n",
    "        # Shuffle the order of mini batches for each epoch\n",
    "        np.random.shuffle(mini_batches)  \n",
    "        for (mini_batch_X, mini_batch_y) in mini_batches:\n",
    "\n",
    "            # compute the predictions for the mini_batch\n",
    "            xs, p, s, s_hat, temp_means, temp_var = evaluate_classifier_BN(mini_batch_X, Ws_star, bs_star, np.zeros(k), np.zeros(k), gamma_star, beta_star, 0)\n",
    "            \n",
    "            # update means and var\n",
    "            if step_t == 0:\n",
    "                means = temp_means\n",
    "                var = temp_var\n",
    "            else:\n",
    "                for i in range(0, len(means)):\n",
    "                    means[i] = means[i]*alpha + temp_means[i] * (1-alpha)\n",
    "                    var[i] = var[i] * alpha + temp_var[i] *(1-alpha)\n",
    "            \n",
    "            # update lr\n",
    "            step_t, eta, l = update_cyclical_lr(step_size, l, eta_min, eta_max, step_t)\n",
    "            \n",
    "            # compute the new gradients\n",
    "            grad_Ws, grad_bs, grad_gamma, grad_beta = compute_gradient(mini_batch_y, p, Ws_star, xs, s, s_hat, gamma_star, means, var, lbd)\n",
    "\n",
    "            \n",
    "            for i in range(0, len(Ws_star)):\n",
    "                Ws_star[i] -= eta * grad_Ws[i]\n",
    "                bs_star[i] -= eta * grad_bs[i]\n",
    "            for i in range(0, len(gamma_star)):\n",
    "                gamma_star[i] -= eta * grad_gamma[i]\n",
    "                beta_star[i] -= eta * grad_beta[i]\n",
    "            \n",
    "            if step_t % 400 == 0:\n",
    "                # compute the loss and cost function values\n",
    "                cost.append(compute_cost(mini_batch_X, mini_batch_y, Ws_star, bs_star, lbd, gamma_star, beta_star, np.zeros(k), np.zeros(k), 0))\n",
    "                cost_vald.append(compute_cost(X_validation, Y_validation, Ws_star, bs_star, lbd, gamma_star, beta_star, np.zeros(k), np.zeros(k), 0))\n",
    "\n",
    "        #accuracy.append(compute_accuracy(mini_batch_X, np.argmax(mini_batch_y, axis=0), W1_star, b1_star, W2_star, b2_star))\n",
    "        #accuracy_vald.append(compute_accuracy(X_validation, np.argmax(Y_validation, axis=0), W1_star, b1_star, W2_star, b2_star))\n",
    "    \n",
    "    plot_loss_cost(cost, cost_vald)\n",
    "    #plot_accuracy(accuracy, accuracy_vald)\n",
    "    return Ws_star, bs_star, gamma_star, beta_star, means, var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb80ac6",
   "metadata": {},
   "source": [
    "Auxiliary functions needed by mini_batch_gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "454aabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return a tuple of arrays (x_batch, y_batch)\n",
    "def construct_mini_batches(s_batch, data, onehot_labels):\n",
    "    nb_batch = int(np.ceil(len(data[0])/s_batch))\n",
    "    \n",
    "    mini_batches = []\n",
    "    \n",
    "    for j in range(nb_batch):\n",
    "        # set the start and end index of the batch\n",
    "        j_start = j*s_batch\n",
    "        j_end = (j+1)*s_batch        \n",
    "        x_batch = data[:,j_start:j_end]\n",
    "        y_batch = onehot_labels[:,j_start:j_end]\n",
    "        \n",
    "        mini_batches.append((x_batch, y_batch))\n",
    "        \n",
    "    return mini_batches\n",
    "\n",
    "# function to update learning rate following the cyclical algorithm\n",
    "def update_cyclical_lr(step_size, l, eta_min, eta_max, step_t):\n",
    "    # compute the learning rate\n",
    "    if 2*l*step_size <= step_t <= (2*l + 1)*step_size:\n",
    "        eta = eta_min + (eta_max - eta_min) * (step_t-2*l*step_size) / step_size\n",
    "    else:\n",
    "        eta = eta_max - (eta_max - eta_min) * (step_t-(2*l + 1)*step_size) / step_size\n",
    "        \n",
    "    step_t += 1\n",
    "    if step_t % (step_size*2) == 0:\n",
    "        l += 1\n",
    "    return step_t, eta, l\n",
    "\n",
    "# plot the cost function values after each epoch\n",
    "def plot_loss_cost(cost, cost_vald):\n",
    "    step_size = 100\n",
    "    x_axis = [i*step_size for i in range(len(cost))] # get the step size values as the x-axis\n",
    "    plt.plot(x_axis, cost, label='Train cost')\n",
    "    plt.plot(x_axis, cost_vald, label='Validation cost')\n",
    "    plt.xlabel('step_size')\n",
    "    plt.ylabel('cost')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# plot accuracy\n",
    "def plot_accuracy(accuracy, accuracy_vald):\n",
    "    step_size = 100\n",
    "    x_axis = [i*step_size for i in range(len(accuracy))] # get the step size values as the x-axis\n",
    "    plt.plot(x_axis, accuracy, label='Train')\n",
    "    plt.plot(x_axis, accuracy_vald, label='Validation')\n",
    "    plt.xlabel('step_size')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dca6ff",
   "metadata": {},
   "source": [
    "# Train and test the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e82f486",
   "metadata": {},
   "source": [
    "We know load all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98e1efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir of data\n",
    "data_batch_1 = \"cifar-10-batches-py/data_batch_1\"\n",
    "data_batch_2 = \"cifar-10-batches-py/data_batch_2\"\n",
    "data_batch_3 = \"cifar-10-batches-py/data_batch_3\"\n",
    "data_batch_4 = \"cifar-10-batches-py/data_batch_4\"\n",
    "data_batch_5 = \"cifar-10-batches-py/data_batch_5\"\n",
    "test_batch = \"cifar-10-batches-py/test_batch\"\n",
    "\n",
    "data_1, onehot_labels_1, labels_1 = LoadBatch(data_batch_1)\n",
    "data_2, onehot_labels_2, labels_2 = LoadBatch(data_batch_2)\n",
    "data_3, onehot_labels_3, labels_3 = LoadBatch(data_batch_3)\n",
    "data_4, onehot_labels_4, labels_4 = LoadBatch(data_batch_4)\n",
    "t_data_5, t_onehot_labels_5, t_labels_5 = LoadBatch(data_batch_5)\n",
    "data_test, onehot_labels_test, labels_test = LoadBatch(test_batch)\n",
    "\n",
    "data_5, onehot_labels_5, labels_5 = t_data_5[:,0:5000], t_onehot_labels_5[:,0:5000], t_labels_5[0:5000]\n",
    "data_validation, onehot_labels_validation, labels_validation = t_data_5[:,5000:10000], t_onehot_labels_5[:,5000:10000], t_labels_5[5000:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb120c7",
   "metadata": {},
   "source": [
    "Normalize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af560b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all the data\n",
    "data = np.hstack((data_1, data_2, data_3, data_4, data_5))\n",
    "onehot_labels = np.hstack((onehot_labels_1, onehot_labels_2, onehot_labels_3, onehot_labels_4, onehot_labels_5))\n",
    "labels = np.hstack((labels_1, labels_2, labels_3, labels_4, labels_5))\n",
    "\n",
    "# normalize\n",
    "data = normalize_data(data)\n",
    "data_validation = normalize_data(data_validation)\n",
    "data_test = normalize_data(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc017f1",
   "metadata": {},
   "source": [
    "Train and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea3d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd_params = (s_batch, eta_min, eta_max, step_size, n_epochs)\n",
    "n_cycles = 3\n",
    "s_batch = 100\n",
    "step_size = 45000*5/s_batch\n",
    "n_epochs = (n_cycles * 2 * step_size)/(45000/s_batch)\n",
    "\n",
    "gd_params = (s_batch, 1e-3, 1e-1, step_size, int(n_epochs))\n",
    "gamma = [np.ones(m[i]) for i in range(k-1)]\n",
    "beta = [np.zeros(m[i]) for i in range(k-1)]\n",
    "Ws, bs = initialize_Ws(k, m, data_dimension, labels_count)\n",
    "\n",
    "lbd = 0.005\n",
    "\n",
    "Ws_star, bs_star, gamma_star, beta_star, means, var = mini_batch_gd(data, onehot_labels, gd_params, Ws, bs, gamma, beta, lbd, data_validation, onehot_labels_validation)\n",
    "\n",
    "compute_accuracy(data_validation, labels_validation, Ws_star, bs_star, gamma_star, beta_star, means, var, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d2e32",
   "metadata": {},
   "source": [
    "# Computation of optimal lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81946557",
   "metadata": {},
   "source": [
    "Compute random values of lambda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b00b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_lbd = 0.005\n",
    "\n",
    "# gd_params = (s_batch, eta_min, eta_max, step_size, n_epochs)\n",
    "n_cycles = 3\n",
    "s_batch = 100\n",
    "step_size = 45000*5/s_batch\n",
    "n_epochs = (n_cycles * 2 * step_size)/(45000/s_batch)\n",
    "gd_params = (s_batch, 1e-5, 1e-1, step_size, int(n_epochs))\n",
    "best_accuracy = 0\n",
    "\n",
    "for i in range(8):\n",
    "    # generate random lambda\n",
    "    lbd = np.random.normal(best_lbd, best_lbd*0.001)\n",
    "    Ws, bs = initialize_Ws(k, m, data_dimension, labels_count)\n",
    "    Ws_star, bs_star, gamma_star, beta_star, means, var = mini_batch_gd(data, onehot_labels, gd_params, Ws, bs, gamma, beta, lbd, data_validation, onehot_labels_validation)\n",
    "    accuracy = compute_accuracy(data_validation, labels_validation, Ws_star, bs_star, gamma_star, beta_star, means, var, 1)\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    print(\"lambda: \", lbd)\n",
    "    # Save the best lambda \n",
    "    if(accuracy > best_accuracy):\n",
    "        best_accuracy = accuracy\n",
    "        best_lbd = lbd\n",
    "\n",
    "# gd_params = (s_batch, eta_min, eta_max, step_size, n_epochs)\n",
    "n_cycles = 4\n",
    "s_batch = 100\n",
    "step_size = 45000*5/s_batch\n",
    "n_epochs = (n_cycles * 2 * step_size)/(45000/s_batch)\n",
    "gd_params = (s_batch, 1e-5, 1e-1, step_size, int(n_epochs))\n",
    "\n",
    "for i in range(8):\n",
    "    # generate random lambda\n",
    "    lbd = np.random.normal(best_lbd, best_lbd*0.01)\n",
    "    Ws, bs = initialize_Ws(k, m, data_dimension, labels_count)\n",
    "    Ws_star, bs_star, gamma_star, beta_star, means, var = mini_batch_gd(data, onehot_labels, gd_params, Ws, bs, gamma, beta, lbd, data_validation, onehot_labels_validation)\n",
    "    accuracy = compute_accuracy(data_validation, labels_validation, Ws_star, bs_star, gamma_star, beta_star, means, var, 1)\n",
    "    print(\"accuracy: \", accuracy)\n",
    "    print(\"lambda: \", lbd)\n",
    "    # Save the best lambda \n",
    "    if(accuracy > best_accuracy):\n",
    "        best_accuracy = accuracy\n",
    "        best_lbd = lbd\n",
    "        \n",
    "print(\"best accuracy:\", best_accuracy)\n",
    "print(\"best lambda:\", best_lbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca68d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd_params = (s_batch, eta_min, eta_max, step_size, n_epochs)\n",
    "n_cycles = 2\n",
    "s_batch = 100\n",
    "step_size = 45000*5/s_batch\n",
    "n_epochs = (n_cycles * 2 * step_size)/(45000/s_batch)\n",
    "\n",
    "gd_params = (s_batch, 1e-5, 1e-1, step_size, int(n_epochs))\n",
    "gamma = [np.ones(m[i]) for i in range(k-1)]\n",
    "beta = [np.zeros(m[i]) for i in range(k-1)]\n",
    "\n",
    "lbd = best_lbd\n",
    "Ws_star, bs_star, gamma_star, beta_star, means, var = mini_batch_gd(data, onehot_labels, gd_params, Ws, bs, gamma, beta, lbd, data_validation, onehot_labels_validation)\n",
    "\n",
    "compute_accuracy(data_validation, labels_validation, Ws_star, bs_star, gamma_star, beta_star, means, var, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0752a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
