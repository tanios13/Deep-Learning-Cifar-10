{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe49c202",
   "metadata": {},
   "source": [
    "# Multi-linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3085c3f3",
   "metadata": {},
   "source": [
    "We will train and test a two layer network with multiple outputs to classify images from the CIFAR-10 dataset. We will train the network using mini-batch gradient descent applied to a cost function that computes the cross-entropy loss of the classifier applied to the labelled training data and an L2 regularization term on the weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad740786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions to load batch, softmax function, compute gradient, display image for each label\n",
    "# and transfer model to matlab\n",
    "import functions as functions\n",
    "\n",
    "import tensorflow.keras.utils as np_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e817452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir of data\n",
    "data_batch_1 = \"cifar-10-batches-py/data_batch_1\"\n",
    "# validation data\n",
    "data_batch_2 = \"cifar-10-batches-py/data_batch_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9dcebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter of the network\n",
    "m = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e68052",
   "metadata": {},
   "source": [
    "# Training the multi-linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67932793",
   "metadata": {},
   "source": [
    "## Read in the data & initialize the parameters of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a73119a",
   "metadata": {},
   "source": [
    "We start by extracting the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1691a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads in the data from a CIFAR-10 batch file \n",
    "# Returns the image and label data in separate files.\n",
    "def LoadBatch(filename):\n",
    "    data_dict = functions.load_batch(filename)\n",
    "    data = data_dict[b'data']\n",
    "    \n",
    "    # extract the labels\n",
    "    labels = data_dict[b'labels']\n",
    "    \n",
    "    # convert to one-hot representation\n",
    "    onehot_labels = np_utils.to_categorical(labels)\n",
    "    \n",
    "    return data.T, onehot_labels.T, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74776ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is (data_dimension, data_count)\n",
    "# onehot_labels is (labels_count, data_count)\n",
    "data, onehot_labels, labels = LoadBatch(data_batch_1)\n",
    "\n",
    "# we do the same for the validation set\n",
    "X_validation, Y_validation, labels_validation = LoadBatch(data_batch_2)\n",
    "\n",
    "data_dimension = len(data[:,0])\n",
    "labels_count = len(onehot_labels[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3c31ae",
   "metadata": {},
   "source": [
    "Now we preprocess the raw data by normalizing it (we assume the noise is Gaussian and that the data is normally distributed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375919e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(X):    \n",
    "    return (X -  np.mean(X,axis=1)[:, np.newaxis]) / np.std(X, axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62d0ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Than normalize the data according to the normal distribution\n",
    "data = normalize_data(data)\n",
    "X_validation = normalize_data(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5485711",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"data mean:\", np.mean(data))\n",
    "print(\"data std:\", np.std(data))\n",
    "\n",
    "print(\"validation mean:\", np.mean(data))\n",
    "print(\"validation std:\", np.std(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c219fe3",
   "metadata": {},
   "source": [
    "We know $$ s_1 = W_1x + b_1$$\n",
    "$$h = max(0,s_1)$$\n",
    "$$s = W_2h + b_2$$\n",
    "$$p = softmax(s)$$\n",
    "initialize the W1, W2, b1 and b2 of the model with each entry have Gaussian random values with zero mean and standard\n",
    "deviation .01 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd9eb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the shape of W\n",
    "W1_shape = (m, data_dimension)\n",
    "b1_shape = (m, 1)\n",
    "W2_shape = (labels_count, m)\n",
    "b2_shape = (labels_count, 1)\n",
    "\n",
    "# Initialize the array with Gaussian random values\n",
    "W1 = np.random.normal(loc=0.0, scale=1/np.sqrt(data_dimension), size=W1_shape)\n",
    "W2 = np.random.normal(loc=0.0, scale=1/np.sqrt(m), size=W2_shape)\n",
    "b1 = b2 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1972ce48",
   "metadata": {},
   "source": [
    "## Compute the gradients for the network parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a19297",
   "metadata": {},
   "source": [
    "We write the evaluation and cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677ab3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a (label_count, data_count) matrix\n",
    "def evaluate_classifier(data, W1, b1, W2, b2):\n",
    "    h = np.maximum(0, W1@data + b1)\n",
    "    p = functions.softmax(W2@h + b2)\n",
    "    return h, p\n",
    "\n",
    "# returns the cost functions that we need to minimize\n",
    "def compute_cost(data, onehot_labels, W1, b1, W2, b2, lbd):\n",
    "    cost = 0\n",
    "    \n",
    "    # p is (label_count, data_count)\n",
    "    h, p = evaluate_classifier(data, W1, b1, W2, b2)\n",
    "    \n",
    "    # for every data in training data set\n",
    "    for d in range(0,len(data[0])):\n",
    "        cost -= onehot_labels[:,d].T @ np.log(p[:,d])\n",
    "        \n",
    "    # we devide by the data_size    \n",
    "    cost /= len(data[0])\n",
    "    \n",
    "    # we add the regularization term\n",
    "    cost += lbd * (np.sum(W1**2) + np.sum(W2**2))\n",
    "         \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f6d65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compute_cost(data, onehot_labels, W1, b1, W2, b2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b64692",
   "metadata": {},
   "source": [
    "We write a function that computes the accuracy of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4995fd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(data, labels, W1, b1, W2, b2):    \n",
    "    # Get the index of the maximum value which is the label for each row\n",
    "    h , p = evaluate_classifier(data, W1, b1, W2, b2)\n",
    "    predicted_labels = np.argmax(p, axis=0)\n",
    "    \n",
    "    return np.sum(labels == predicted_labels) / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc382be",
   "metadata": {},
   "source": [
    "We compute the accuracy for the randomly initialized parameters. We should get an accuracy of 10% since it's random and there is 10 labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e32438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(data, labels, W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9cd4cc",
   "metadata": {},
   "source": [
    "We write the function that evaluates for a mini-batch the gradient of the cost function.\n",
    "\n",
    "The mini-batch gradient is defined as follows: \n",
    "\n",
    "$$\\textbf{W}^{t+1} = \\textbf{W}^t - \\eta \\sum_{n \\in B^t} \\nabla l_{cross}(\\textbf{x},\\textbf{y},\\textbf{W},\\textbf{b})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06e297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini batch data is (data_dimension, data_size) \n",
    "# onehot_labels is (label_count, data_size)\n",
    "# predicted_labels is (label_count, data_size)\n",
    "# W is (label_count, data_dimension)\n",
    "def compute_gradient(data, onehot_labels, predicted_labels, W1, W2, H, lbd):\n",
    "    data_size = len(data[0])\n",
    "        \n",
    "    # g is (label_count, data_size)\n",
    "    # We start by the last layer\n",
    "    g = -(onehot_labels - predicted_labels)\n",
    "    \n",
    "    grad_W2 = 2*lbd*W2 + (g @ H.T)/data_size\n",
    "    grad_b2 = np.mean(g, axis = 1)\n",
    "    \n",
    "    # Now the first layer\n",
    "    g = W2.T @ g\n",
    "    g = g * (H > 0).astype(int)\n",
    "    \n",
    "    grad_W1 = 2*lbd*W1 + (g @ data.T)/data_size\n",
    "    grad_b1 = np.mean(g, axis = 1)\n",
    "        \n",
    "    return grad_W1, grad_b1[:, np.newaxis], grad_W2, grad_b2[:, np.newaxis] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200487e0",
   "metadata": {},
   "source": [
    "## Train the network with cyclical learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df71d3d",
   "metadata": {},
   "source": [
    "Now that we made sure the gradient descent it correct, we implement the mini batch gardient algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0224f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd_params = (s_batch, eta, n_epochs)\n",
    "def mini_batch_gd(data, onehot_labels, gd_params, W1, b1, W2, b2, lbd, X_validation, Y_validation):\n",
    "    # define the parameters\n",
    "    s_batch = gd_params[0]\n",
    "    eta_min, eta_max, step_size = gd_params[1], gd_params[2], gd_params[3]\n",
    "    n_epochs = gd_params[4]\n",
    "    \n",
    "    # initialize empty lists to store the loss and cost function values\n",
    "    cost = []\n",
    "    cost_vald = []\n",
    "    accuracy = []\n",
    "    accuracy_vald = []\n",
    "\n",
    "    W1_star = W1\n",
    "    b1_star = b1\n",
    "    W2_star = W2\n",
    "    b2_star = b2\n",
    "    \n",
    "    # cyclical learning rate\n",
    "    step_t = 0\n",
    "    l = 0\n",
    "    \n",
    "    # construct the mini batches\n",
    "    mini_batches = construct_mini_batches(s_batch, data, onehot_labels)\n",
    "        \n",
    "    validation_mini_batches = construct_mini_batches(s_batch, X_validation, Y_validation)\n",
    "     \n",
    "    for iter in range(n_epochs):\n",
    "        for (mini_batch_X, mini_batch_y) in mini_batches:\n",
    "            # compute the predictions for the mini_batch\n",
    "            h, p = evaluate_classifier(mini_batch_X, W1_star, b1_star, W2_star, b2_star)\n",
    "            \n",
    "            # compute the learning rate\n",
    "            if 2*l*step_size <= step_t <= (2*l + 1)*step_size:\n",
    "                eta = eta_min + (eta_max - eta_min) * (step_t-2*l*step_size) / step_size\n",
    "            else:\n",
    "                eta = eta_max - (eta_max - eta_min) * (step_t-(2*l + 1)*step_size) / step_size\n",
    "\n",
    "            step_t += 1\n",
    "            if step_t % (step_size*2) == 0:\n",
    "                l += 1\n",
    "            \n",
    "            # compute the new gradients\n",
    "            grad_W1, grad_b1, grad_W2, grad_b2 = compute_gradient(mini_batch_X, mini_batch_y, p, W1_star, W2_star, h, lbd)        \n",
    "            W1_star = W1_star - eta * grad_W1\n",
    "            b1_star = b1_star - eta * grad_b1\n",
    "            W2_star = W2_star - eta * grad_W2\n",
    "            b2_star = b2_star - eta * grad_b2\n",
    "            \n",
    "            if step_t % 400 == 0:\n",
    "                # compute the loss and cost function values\n",
    "                cost.append(compute_cost(mini_batch_X, mini_batch_y, W1_star, b1_star, W2_star, b2_star, lbd))\n",
    "                cost_vald.append(compute_cost(X_validation, Y_validation, W1_star, b1_star, W2_star, b2_star, lbd))\n",
    "\n",
    "        #accuracy.append(compute_accuracy(mini_batch_X, np.argmax(mini_batch_y, axis=0), W1_star, b1_star, W2_star, b2_star))\n",
    "        #accuracy_vald.append(compute_accuracy(X_validation, np.argmax(Y_validation, axis=0), W1_star, b1_star, W2_star, b2_star))\n",
    "    \n",
    "    print(l)\n",
    "    plot_loss_cost(cost, cost_vald)\n",
    "    #plot_accuracy(accuracy, accuracy_vald)\n",
    "    return W1_star, b1_star, W2_star, b2_star\n",
    "    \n",
    "# return a tuple of arrays (x_batch, y_batch)\n",
    "def construct_mini_batches(s_batch, data, onehot_labels):\n",
    "    nb_batch = int(np.ceil(len(data[0])/s_batch))\n",
    "    \n",
    "    mini_batches = []\n",
    "    \n",
    "    for j in range(nb_batch):\n",
    "        # set the start and end index of the batch\n",
    "        j_start = j*s_batch\n",
    "        j_end = (j+1)*s_batch        \n",
    "        x_batch = data[:,j_start:j_end]\n",
    "        y_batch = onehot_labels[:,j_start:j_end]\n",
    "        \n",
    "        mini_batches.append((x_batch, y_batch))\n",
    "        \n",
    "    return mini_batches\n",
    "\n",
    "# plot the cost function values after each epoch\n",
    "def plot_loss_cost(cost, cost_vald):\n",
    "    step_size = 100\n",
    "    x_axis = [i*step_size for i in range(len(cost))] # get the step size values as the x-axis\n",
    "    plt.plot(x_axis, cost, label='Train cost')\n",
    "    plt.plot(x_axis, cost_vald, label='Validation cost')\n",
    "    plt.xlabel('step_size')\n",
    "    plt.ylabel('cost')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# plot accuracy\n",
    "def plot_accuracy(accuracy, accuracy_vald):\n",
    "    step_size = 100\n",
    "    x_axis = [i*step_size for i in range(len(accuracy))] # get the step size values as the x-axis\n",
    "    plt.plot(x_axis, accuracy, label='Train')\n",
    "    plt.plot(x_axis, accuracy_vald, label='Validation')\n",
    "    plt.xlabel('step_size')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46efebb",
   "metadata": {},
   "source": [
    "We test the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942a2717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd_params = (s_batch, eta_min, eta_max, step_size, n_epochs)\n",
    "gd_params = (100, 1e-5, 1e-1, 500, 10)\n",
    "lbd = 0.01\n",
    "W1_star, b1_star, W2_star, b2_star = mini_batch_gd(data, onehot_labels, gd_params, W1, b1, W2, b2, lbd, X_validation, Y_validation)\n",
    "\n",
    "compute_accuracy(X_validation, labels_validation, W1_star, b1_star, W2_star, b2_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b26ca2c",
   "metadata": {},
   "source": [
    "## Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe76bc",
   "metadata": {},
   "source": [
    "We run it for more than 1 cycle (say 3), and for a larger step size $n_s = 800$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b396358",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gd_params = (s_batch, eta_min, eta_max, step_size, n_epochs)\n",
    "gd_params = (100, 1e-5, 1e-1, 800, 48)\n",
    "lbd = 0\n",
    "W1_star, b1_star, W2_star, b2_star = mini_batch_gd(data, onehot_labels, gd_params, W1, b1, W2, b2, lbd, X_validation, Y_validation)\n",
    "\n",
    "compute_accuracy(X_validation, labels_validation, W1_star, b1_star, W2_star, b2_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e82f486",
   "metadata": {},
   "source": [
    "Load all the data for the coarse search of lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e1efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir of data\n",
    "data_batch_1 = \"cifar-10-batches-py/data_batch_1\"\n",
    "data_batch_2 = \"cifar-10-batches-py/data_batch_2\"\n",
    "data_batch_3 = \"cifar-10-batches-py/data_batch_3\"\n",
    "data_batch_4 = \"cifar-10-batches-py/data_batch_4\"\n",
    "data_batch_5 = \"cifar-10-batches-py/data_batch_5\"\n",
    "test_batch = \"cifar-10-batches-py/test_batch\"\n",
    "\n",
    "data_1, onehot_labels_1, labels_1 = LoadBatch(data_batch_1)\n",
    "data_2, onehot_labels_2, labels_2 = LoadBatch(data_batch_2)\n",
    "data_3, onehot_labels_3, labels_3 = LoadBatch(data_batch_3)\n",
    "data_4, onehot_labels_4, labels_4 = LoadBatch(data_batch_4)\n",
    "t_data_5, t_onehot_labels_5, t_labels_5 = LoadBatch(data_batch_5)\n",
    "data_test, onehot_labels_test, labels_test = LoadBatch(test_batch)\n",
    "\n",
    "data_5, onehot_labels_5, labels_5 = t_data_5[:,0:9000], t_onehot_labels_5[:,0:9000], t_labels_5[0:9000]\n",
    "data_validation, onehot_labels_validation, labels_validation = t_data_5[:,9000:10000], t_onehot_labels_5[:,9000:10000], t_labels_5[9000:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af560b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all the data\n",
    "data = np.hstack((data_1, data_2, data_3, data_4, data_5))\n",
    "onehot_labels = np.hstack((onehot_labels_1, onehot_labels_2, onehot_labels_3, onehot_labels_4, onehot_labels_5))\n",
    "\n",
    "# normalize\n",
    "data = normalize_data(data)\n",
    "data_validation = normalize_data(data_validation)\n",
    "data_test = normalize_data(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81946557",
   "metadata": {},
   "source": [
    "Compute random values of lambda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b00b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l_min, l_max = -5, -1\n",
    "\n",
    "# gd_params = (s_batch, eta_min, eta_max, step_size, n_epochs)\n",
    "gd_params = (100, 1e-5, 1e-1, 980, 8)\n",
    "best_accuracy = 0\n",
    "\n",
    "for i in range(8):\n",
    "    # generate random lambda\n",
    "    l = l_min + (l_max - l_min)*np.random.rand(1, 1)\n",
    "    lbd = (10 ** l) [0][0]\n",
    "    \n",
    "    W1_star, b1_star, W2_star, b2_star = mini_batch_gd(data, onehot_labels, gd_params, W1, b1, W2, b2, lbd, data_validation, onehot_labels_validation)\n",
    "    accuracy = compute_accuracy(data_validation, labels_validation, W1_star, b1_star, W2_star, b2_star)\n",
    "    \n",
    "    # Save the best lambda \n",
    "    if(accuracy > best_accuracy):\n",
    "        best_accuracy = accuracy\n",
    "        best_lbd = lbd\n",
    "\n",
    "# gd_params = (s_batch, eta_min, eta_max, step_size, n_epochs)\n",
    "gd_params = (100, 1e-5, 1e-1, 980, 16)\n",
    "\n",
    "for i in range(8):\n",
    "    # generate random lambda\n",
    "    lbd = np.random.normal(best_lbd, best_lbd*0.01)\n",
    "    \n",
    "    W1_star, b1_star, W2_star, b2_star = mini_batch_gd(data, onehot_labels, gd_params, W1, b1, W2, b2, lbd, data_validation, onehot_labels_validation)\n",
    "    accuracy = compute_accuracy(data_validation, labels_validation, W1_star, b1_star, W2_star, b2_star)\n",
    "\n",
    "    # Save the best lambda \n",
    "    if(accuracy > best_accuracy):\n",
    "        best_accuracy = accuracy\n",
    "        best_lbd = lbd\n",
    "        \n",
    "print(\"best accuracy:\", best_accuracy)\n",
    "print(\"best lambda:\", best_lbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca68d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gd_params = (s_batch, eta_min, eta_max, step_size, n_epochs)\n",
    "gd_params = (100, 1e-5, 1e-1, 980, 12)\n",
    "lbd = best_lbd\n",
    "W1_star, b1_star, W2_star, b2_star = mini_batch_gd(data, onehot_labels, gd_params, W1, b1, W2, b2, lbd, data_validation, onehot_labels_validation)\n",
    "\n",
    "compute_accuracy(data_test, labels_test, W1_star, b1_star, W2_star, b2_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0752a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
